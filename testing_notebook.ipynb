{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d8a558de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "import httpx\n",
    "from urllib.parse import quote\n",
    "from bs4 import BeautifulSoup\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1d485b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ARMY_PUBS_API_BASE = \"https://armypubs.army.mil/ProductMaps/PubForm/ContentSearch.aspx\"\n",
    "USER_AGENT = \"microsoft-army-pubs-mcp/0.1\"\n",
    "\n",
    "async def make_pubs_request(url: str) -> str | None:\n",
    "    \"\"\"Make a request to the Army publications API with proper error handling.\"\"\"\n",
    "    headers = {\n",
    "        \"User-Agent\": USER_AGENT,\n",
    "        \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\n",
    "    }\n",
    "    async with httpx.AsyncClient() as client:\n",
    "        try:\n",
    "            print(f\"(make_pubs_request) Requesting Army publications API: {url}\")\n",
    "            response = await client.get(url, headers=headers, timeout=30.0)\n",
    "            print(f\"(make_pubs_request) Response status code: {response.status_code}\")\n",
    "            response.raise_for_status()\n",
    "            return response.text\n",
    "        except Exception as e:\n",
    "            print(f\"(make_pubs_request) Error making request: {e}\")\n",
    "            return None\n",
    "\n",
    "def parse_search_results(html_content: str) -> list[dict[str, Any]]:\n",
    "    \"\"\"Parse the HTML search results and extract reference data.\"\"\"\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    results = []\n",
    "    \n",
    "    # Find the results table\n",
    "    results_table = soup.find('div', {'id': 'MainContent_tblContentSearchResults'})\n",
    "    if not results_table:\n",
    "        print(\"(parse_search_results) No results table found\")\n",
    "        return results\n",
    "    \n",
    "    # Find all publication entries\n",
    "    links = results_table.find_all('a', href=True)\n",
    "    \n",
    "    for i, link in enumerate(links):\n",
    "        if 'epubs' in link['href'] or 'pub/eforms' in link['href']:\n",
    "            # Extract title and document type\n",
    "            title_text = link.get_text(strip=True)\n",
    "            \n",
    "            # Skip \"Record Details\" links\n",
    "            if \"Record Details\" in title_text:\n",
    "                continue\n",
    "                \n",
    "            # Parse title to extract document number and name\n",
    "            title_parts = title_text.split(' — ', 1)\n",
    "            doc_number = title_parts[0] if title_parts else title_text\n",
    "            doc_title = title_parts[1] if len(title_parts) > 1 else \"\"\n",
    "            \n",
    "            # Extract document type from number (e.g., \"TC\", \"AR\", \"ATP\", etc.)\n",
    "            doc_type_match = re.match(r'^([A-Z]+)', doc_number)\n",
    "            doc_type = doc_type_match.group(1) if doc_type_match else \"Unknown\"\n",
    "            \n",
    "            # Get file format from the span after the link\n",
    "            file_format = \"pdf\"  # default\n",
    "            next_span = link.find_next('span')\n",
    "            if next_span and 'font-size:smaller' in str(next_span):\n",
    "                file_format = next_span.get_text(strip=True)\n",
    "            \n",
    "            # Find the description/date text (next <td> after the link)\n",
    "            description_td = link.find_parent('td')\n",
    "            if description_td:\n",
    "                next_td = description_td.find_next_sibling('td')\n",
    "                if not next_td:\n",
    "                    # Look for the next row\n",
    "                    next_tr = description_td.find_parent('tr').find_next_sibling('tr')\n",
    "                    if next_tr:\n",
    "                        next_td = next_tr.find('td')\n",
    "                \n",
    "                date_text = \"\"\n",
    "                description = \"\"\n",
    "                if next_td:\n",
    "                    full_text = next_td.get_text(strip=True)\n",
    "                    \n",
    "                    # Extract date (look for pattern like \"May 13, 2019\" or \"Feb 11, 2025\")\n",
    "                    date_match = re.search(r'(\\w{3}\\s+\\d{1,2},\\s+\\d{4})', full_text)\n",
    "                    if date_match:\n",
    "                        date_text = date_match.group(1)\n",
    "                    \n",
    "                    # Clean up description text\n",
    "                    description = full_text\n",
    "                    \n",
    "                    # Handle CAC-required documents\n",
    "                    if \"Common Access Card (CAC) to view it\" in full_text:\n",
    "                        description = \"This publication or form requires Common Access Card (CAC) to view it\"\n",
    "            \n",
    "            result = {\n",
    "                \"document_number\": doc_number,\n",
    "                \"title\": doc_title,\n",
    "                \"document_type\": doc_type,\n",
    "                \"file_format\": file_format,\n",
    "                \"date\": date_text,\n",
    "                \"description\": description,\n",
    "                \"url\": link['href'] if link['href'].startswith('http') else f\"https://armypubs.army.mil{link['href']}\"\n",
    "            }\n",
    "            \n",
    "            results.append(result)\n",
    "    \n",
    "    return results\n",
    "        \n",
    "async def search_pubs(query: str) -> list[dict[str, Any]]:\n",
    "    \"\"\"Get a specific document from the Army publications website.\n",
    "\n",
    "    Args:\n",
    "        query: any string to search for in the Army publications\n",
    "    \"\"\"\n",
    "    encoded_query = quote(query)\n",
    "    url = f\"{ARMY_PUBS_API_BASE}?q={encoded_query}\"\n",
    "    print(f\"(search_pubs) Search URL: {url}\")\n",
    "    \n",
    "    html_content = await make_pubs_request(url)\n",
    "    if not html_content:\n",
    "        return []\n",
    "    \n",
    "    # Parse the HTML and extract structured data\n",
    "    results = parse_search_results(html_content)\n",
    "    \n",
    "    print(f\"(search_pubs) Found {len(results)} publications\")\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7835d69c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(search_pubs) Search URL: https://armypubs.army.mil/ProductMaps/PubForm/ContentSearch.aspx?q=artificial%20intelligence\n",
      "(make_pubs_request) Requesting Army publications API: https://armypubs.army.mil/ProductMaps/PubForm/ContentSearch.aspx?q=artificial%20intelligence\n",
      "(make_pubs_request) Response status code: 200\n",
      "(search_pubs) Found 10 publications\n"
     ]
    }
   ],
   "source": [
    "# Test the search function (commented out to avoid duplicate output)\n",
    "results = await search_pubs(\"artificial intelligence\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d79d5a53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1. ARMY DIR 2018-18 — ARMY ARTIFICIAL INTELLIGENCE TASK FORCE IN SUPPORT OF THE DEPARTMENT OF DEFENSE JOINT ARTIFICIAL INTELLIGENCE CENTER\n",
      "   Type: ARMY | Format: pdf\n",
      "   Date: Oct 02, 2018\n",
      "   Description: This publication or form requires Common Access Card (CAC) to view it\n",
      "   URL: https://armypubs.army.mil/epubs/DR_pubs/DR_c/ARN13011-ARMY_DIR_2018-18-000-WEB-1.pdf\n",
      "\n",
      "2. PPM CIO-024 — CHIEF INFORMATION OFFICER GUIDANCE ON GENERATIVE ARTIFICIAL INTELLIGENCE AND LARGE LANGUAGE MODELS\n",
      "   Type: PPM | Format: pdf\n",
      "   Date: Jun 27, 2024\n",
      "   Description: Jun 27, 2024 — PPM CIO-024 CHIEF INFORMATION OFFICER GUIDANCE ON GENERATIVEARTIFICIALINTELLIGENCEAND...\n",
      "   URL: https://armypubs.army.mil/epubs/DR_pubs/DR_a/ARN41285-PPM_CIO-024-000-WEB-1.pdf\n",
      "\n",
      "3. SD 13 AIMS 2023 — THE ARMY INTELLIGENCE MODERNIZATION STRATEGY\n",
      "   Type: SD | Format: pdf\n",
      "   Date: Nov 29, 2023\n",
      "   Description: This publication or form requires Common Access Card (CAC) to view it\n",
      "   URL: https://armypubs.army.mil/epubs/DR_pubs/DR_c/ARN39968-SD_13_AIMS_2023-000-WEB-1.pdf\n",
      "\n",
      "4. ADP 2-0 — INTELLIGENCE\n",
      "   Type: ADP | Format: ebook\n",
      "   Date: Jul 31, 2019\n",
      "   Description: Jul 31, 2019 — ADP 2-0INTELLIGENCEADP 2 0INTELLIGENCEThis publication is available at the Army Publi...\n",
      "   URL: https://armypubs.army.mil/epubs/DR_pubs/DR_a/ARN18009-ADP_2-0-000-EBOOK-3.epub\n",
      "\n",
      "5. ADP 2-0 — INTELLIGENCE\n",
      "   Type: ADP | Format: pdf\n",
      "   Date: Jul 31, 2019\n",
      "   Description: Jul 31, 2019 — ADP 2-0INTELLIGENCEADP 2 0INTELLIGENCENote Recommended Changes to Publications and Bl...\n",
      "   URL: https://armypubs.army.mil/epubs/DR_pubs/DR_a/ARN18009-ADP_2-0-000-WEB-2.pdf\n",
      "\n",
      "6. FM 2-0 — INTELLIGENCE\n",
      "   Type: FM | Format: pdf\n",
      "   Date: Oct 01, 2023\n",
      "   Description: Oct 01, 2023 — FM 2-0INTELLIGENCE...\n",
      "   URL: https://armypubs.army.mil/epubs/DR_pubs/DR_a/ARN39259-FM_2-0-000-WEB-2.pdf\n",
      "\n",
      "7. ATP 2-22.4 — TECHNICAL INTELLIGENCE\n",
      "   Type: ATP | Format: pdf\n",
      "   Date: Oct 29, 2021\n",
      "   Description: This publication or form requires Common Access Card (CAC) to view it\n",
      "   URL: https://armypubs.army.mil/epubs/DR_pubs/DR_c/ARN34092-ATP_2-22.4-000-WEB-1.pdf\n",
      "\n",
      "8. ATP 2-22.7 — GEOSPATIAL INTELLIGENCE\n",
      "   Type: ATP | Format: pdf\n",
      "   Date: Jun 09, 2025\n",
      "   Description: Jun 09, 2025 — ATP 2-22.7 GEOSPATIALINTELLIGENCEATP 2 22 7 GEOSPATIALINTELLIGENCEATP 2 22 7 GEOSPATI...\n",
      "   URL: https://armypubs.army.mil/epubs/DR_pubs/DR_a/ARN43925-ATP_2-22.7-000-WEB-1.pdf\n",
      "\n",
      "9. ATP 3-05.20 — SPECIAL OPERATIONS INTELLIGENCE\n",
      "   Type: ATP | Format: pdf\n",
      "   Date: May 03, 2013\n",
      "   Description: This publication or form requires Common Access Card (CAC) to view it\n",
      "   URL: https://armypubs.army.mil/epubs/DR_pubs/DR_c/pdf/web/atp3_05x20.pdf\n",
      "\n",
      "10. ATP 3-39.20 — POLICE INTELLIGENCE OPERATIONS\n",
      "   Type: ATP | Format: pdf\n",
      "   Date: May 13, 2019\n",
      "   Description: May 13, 2019 — ATP 3-39.20 POLICEINTELLIGENCEOPERATIONS ATP 3 39 20 POLICEINTELLIGENCEOPERATIONS ATP...\n",
      "   URL: https://armypubs.army.mil/epubs/DR_pubs/DR_a/pdf/web/ARN17040_ATP 3-39x20 FINAL WEB.pdf\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Display the results in a nice format\n",
    "for i, result in enumerate(results, 1):  # Show results starting from 1\n",
    "    print(f\"\\n{i}. {result['document_number']} — {result['title']}\")\n",
    "    print(f\"   Type: {result['document_type']} | Format: {result['file_format']}\")\n",
    "    if result['date']:\n",
    "        print(f\"   Date: {result['date']}\")\n",
    "    print(f\"   Description: {result['description'][:100]}{'...' if len(result['description']) > 100 else ''}\")\n",
    "    print(f\"   URL: {result['url']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09bb0297",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aa37ba78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "import io\n",
    "import ebooklib\n",
    "from ebooklib import epub\n",
    "from bs4 import BeautifulSoup\n",
    "import mimetypes\n",
    "from pathlib import Path\n",
    "import tempfile\n",
    "import os\n",
    "\n",
    "async def extract_pdf_text(doc_content: bytes | str) -> str | None:\n",
    "    \"\"\"Extract text from a PDF document.\n",
    "    \n",
    "    Args:\n",
    "        doc_content: The PDF content as bytes or the result from make_pubs_request\n",
    "        \n",
    "    Returns:\n",
    "        Extracted text as string, or None if extraction fails\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # If we got a string response from make_pubs_request, we need to fetch the actual PDF\n",
    "        if isinstance(doc_content, str):\n",
    "            print(\"(extract_pdf_text) Content appears to be HTML, not PDF bytes\")\n",
    "            return None\n",
    "            \n",
    "        # Create a PDF reader from bytes\n",
    "        pdf_file = io.BytesIO(doc_content)\n",
    "        pdf_reader = PyPDF2.PdfReader(pdf_file)\n",
    "        \n",
    "        # Extract text from all pages\n",
    "        text = \"\"\n",
    "        for page_num in range(len(pdf_reader.pages)):\n",
    "            page = pdf_reader.pages[page_num]\n",
    "            text += page.extract_text() + \"\\n\"\n",
    "        \n",
    "        print(f\"(extract_pdf_text) Successfully extracted text from {len(pdf_reader.pages)} pages\")\n",
    "        return text.strip()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"(extract_pdf_text) Error extracting PDF text: {e}\")\n",
    "        return None\n",
    "\n",
    "async def extract_epub_text(doc_content: bytes | str) -> str | None:\n",
    "    \"\"\"Extract text from an EPUB document.\n",
    "    \n",
    "    Args:\n",
    "        doc_content: The EPUB content as bytes or the result from make_pubs_request\n",
    "        \n",
    "    Returns:\n",
    "        Extracted text as string, or None if extraction fails\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # If we got a string response from make_pubs_request, we need to fetch the actual EPUB\n",
    "        if isinstance(doc_content, str):\n",
    "            print(\"(extract_epub_text) Content appears to be HTML, not EPUB bytes\")\n",
    "            return None\n",
    "            \n",
    "        # Create a temporary file since ebooklib requires a file path\n",
    "        with tempfile.NamedTemporaryFile(suffix='.epub', delete=False) as temp_file:\n",
    "            temp_file.write(doc_content)\n",
    "            temp_path = temp_file.name\n",
    "        \n",
    "        try:\n",
    "            # Read the EPUB from the temporary file\n",
    "            book = epub.read_epub(temp_path)\n",
    "            \n",
    "            # Extract text from all chapters\n",
    "            text_content = []\n",
    "            \n",
    "            # Get all items that are documents (chapters)\n",
    "            for item in book.get_items():\n",
    "                if item.get_type() == ebooklib.ITEM_DOCUMENT:\n",
    "                    # Parse HTML content and extract text\n",
    "                    soup = BeautifulSoup(item.get_content(), 'html.parser')\n",
    "                    # Remove script and style elements\n",
    "                    for script in soup([\"script\", \"style\"]):\n",
    "                        script.decompose()\n",
    "                    \n",
    "                    # Get text and clean it up\n",
    "                    text = soup.get_text(separator='\\n', strip=True)\n",
    "                    if text.strip():  # Only add non-empty content\n",
    "                        text_content.append(text)\n",
    "            \n",
    "            full_text = \"\\n\\n\".join(text_content)\n",
    "            print(f\"(extract_epub_text) Successfully extracted text from {len(text_content)} chapters\")\n",
    "            return full_text.strip()\n",
    "            \n",
    "        finally:\n",
    "            # Clean up temporary file\n",
    "            try:\n",
    "                os.unlink(temp_path)\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"(extract_epub_text) Error extracting EPUB text: {e}\")\n",
    "        return None\n",
    "\n",
    "async def extract_document_text(doc_content: bytes | str, file_format: str = None, url: str = None) -> str | None:\n",
    "    \"\"\"Extract text from a document (PDF or EPUB) based on format or URL.\n",
    "    \n",
    "    Args:\n",
    "        doc_content: The document content as bytes\n",
    "        file_format: The format hint ('pdf', 'epub', 'ebook', etc.)\n",
    "        url: The URL to help determine format from extension\n",
    "        \n",
    "    Returns:\n",
    "        Extracted text as string, or None if extraction fails\n",
    "    \"\"\"\n",
    "    # Determine format from various sources\n",
    "    format_type = None\n",
    "    \n",
    "    if file_format:\n",
    "        format_lower = file_format.lower()\n",
    "        if 'pdf' in format_lower:\n",
    "            format_type = 'pdf'\n",
    "        elif 'epub' in format_lower or 'ebook' in format_lower:\n",
    "            format_type = 'epub'\n",
    "    \n",
    "    # If no format hint, try to determine from URL\n",
    "    if not format_type and url:\n",
    "        url_lower = url.lower()\n",
    "        if url_lower.endswith('.pdf'):\n",
    "            format_type = 'pdf'\n",
    "        elif url_lower.endswith('.epub'):\n",
    "            format_type = 'epub'\n",
    "        else:\n",
    "            # Try to guess from MIME type\n",
    "            mime_type, _ = mimetypes.guess_type(url)\n",
    "            if mime_type == 'application/pdf':\n",
    "                format_type = 'pdf'\n",
    "            elif mime_type == 'application/epub+zip':\n",
    "                format_type = 'epub'\n",
    "    \n",
    "    # If still no format, try to detect from content\n",
    "    if not format_type and isinstance(doc_content, bytes):\n",
    "        # Check for PDF magic bytes\n",
    "        if doc_content.startswith(b'%PDF'):\n",
    "            format_type = 'pdf'\n",
    "        # Check for EPUB magic bytes (it's a ZIP file)\n",
    "        elif doc_content.startswith(b'PK') and b'epub' in doc_content[:1024].lower():\n",
    "            format_type = 'epub'\n",
    "    \n",
    "    print(f\"(extract_document_text) Detected format: {format_type}\")\n",
    "    \n",
    "    # Extract based on format\n",
    "    if format_type == 'pdf':\n",
    "        return await extract_pdf_text(doc_content)\n",
    "    elif format_type == 'epub':\n",
    "        return await extract_epub_text(doc_content)\n",
    "    else:\n",
    "        print(f\"(extract_document_text) Unsupported or unknown format: {format_type}\")\n",
    "        return None\n",
    "    \n",
    "\n",
    "async def get_document_text_from_url(doc_url: str, file_format: str = None) -> str | None:\n",
    "    \"\"\"Download a document from URL and extract its text content (supports PDF and EPUB).\n",
    "    \n",
    "    Args:\n",
    "        doc_url: URL of the document\n",
    "        file_format: Optional format hint ('pdf', 'epub', 'ebook', etc.)\n",
    "        \n",
    "    Returns:\n",
    "        Extracted text as string, or None if extraction fails\n",
    "    \"\"\"\n",
    "    headers = {\n",
    "        \"User-Agent\": USER_AGENT,\n",
    "        \"Accept\": \"application/pdf,application/epub+zip,*/*\",\n",
    "    }\n",
    "    \n",
    "    async with httpx.AsyncClient() as client:\n",
    "        try:\n",
    "            print(f\"(get_document_text_from_url) Downloading document: {doc_url}\")\n",
    "            response = await client.get(doc_url, headers=headers, timeout=60.0)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            # Check content type\n",
    "            content_type = response.headers.get('content-type', '')\n",
    "            print(f\"(get_document_text_from_url) Content-Type: {content_type}\")\n",
    "            \n",
    "            return await extract_document_text(response.content, file_format, doc_url)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"(get_document_text_from_url) Error downloading/processing document: {e}\")\n",
    "            # Check if the error indicates CAC authentication is required\n",
    "            if \"redirect\" in str(e).lower() and \"federation.eams.army\" in str(e).lower():\n",
    "                return \"CAC_REQUIRED: This document requires Common Access Card (CAC) authentication\"\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "145ed0ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing PDF extraction:\n",
      "(get_document_text_from_url) Downloading document: https://armypubs.army.mil/epubs/DR_pubs/DR_c/ARN13011-ARMY_DIR_2018-18-000-WEB-1.pdf\n",
      "(get_document_text_from_url) Error downloading/processing document: Redirect response '302 Moved Temporarily' for url 'https://armypubs.army.mil/epubs/DR_pubs/DR_c/ARN13011-ARMY_DIR_2018-18-000-WEB-1.pdf'\n",
      "Redirect location: 'https://federation.eams.army.mil/sso/authenticate/?u=https%3a%2f%2farmypubs.army.mil%2fepubs%2fDR_pubs%2fDR_c%2fARN13011-ARMY_DIR_2018-18-000-WEB-1.pdf&m=GET&r=f&p=10275&f=c&x=true'\n",
      "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/302\n",
      "PDF text extracted successfully (76 characters)\n",
      "First 500 characters:\n",
      "CAC_REQUIRED: This document requires Common Access Card (CAC) authentication\n"
     ]
    }
   ],
   "source": [
    "doc_link = \"https://armypubs.army.mil/epubs/DR_pubs/DR_a/ARN41285-PPM_CIO-024-000-WEB-1.pdf\"\n",
    "\n",
    "# doc = await make_pubs_request(doc_link)  # Example call to fetch a specific document\n",
    "\n",
    "\n",
    "\n",
    "# Test with a PDF document\n",
    "doc_link = \"https://armypubs.army.mil/epubs/DR_pubs/DR_a/ARN41285-PPM_CIO-024-000-WEB-1.pdf\"\n",
    "\n",
    "doc_link = \"https://armypubs.army.mil/epubs/DR_pubs/DR_c/ARN13011-ARMY_DIR_2018-18-000-WEB-1.pdf\" # cac required\n",
    "print(\"Testing PDF extraction:\")\n",
    "pdf_text = await get_document_text_from_url(doc_link, \"pdf\")\n",
    "if pdf_text:\n",
    "    print(f\"PDF text extracted successfully ({len(pdf_text)} characters)\")\n",
    "    print(\"First 500 characters:\")\n",
    "    print(pdf_text[:500] + \"...\" if len(pdf_text) > 500 else pdf_text)\n",
    "else:\n",
    "    print(\"Failed to extract PDF text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5c1704fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing EPUB extraction:\n",
      "(get_document_text_from_url) Downloading document: https://armypubs.army.mil/epubs/DR_pubs/DR_a/ARN18009-ADP_2-0-000-EBOOK-3.epub\n",
      "(get_document_text_from_url) Content-Type: application/epub\n",
      "(extract_document_text) Detected format: epub\n",
      "(extract_epub_text) Successfully extracted text from 15 chapters\n",
      "EPUB text extracted successfully (247063 characters)\n",
      "First 500 characters:\n",
      "This publication is available at the Army Publishing Directorate site (\n",
      "https://armypubs.army.mil/\n",
      "), and the Central Army Registry site (\n",
      "https://atiam.train.army.mil/catalog/dashboard\n",
      ").\n",
      "\n",
      "Foreword\n",
      "The future for our Army is challenging. In order to prepare for an unknowable future, the Army must be ready to conduct the full range of military operations, with a focus on large-scale ground combat operations. The Army will operate across multiple domains with unified action partners. We must depl...\n"
     ]
    }
   ],
   "source": [
    "# Test with an EPUB document (found in our search results)\n",
    "epub_link = \"https://armypubs.army.mil/epubs/DR_pubs/DR_a/ARN18009-ADP_2-0-000-EBOOK-3.epub\"\n",
    "print(\"\\nTesting EPUB extraction:\")\n",
    "epub_text = await get_document_text_from_url(epub_link, \"epub\")\n",
    "if epub_text:\n",
    "    print(f\"EPUB text extracted successfully ({len(epub_text)} characters)\")\n",
    "    print(\"First 500 characters:\")\n",
    "    print(epub_text[:500] + \"...\" if len(epub_text) > 500 else epub_text)\n",
    "else:\n",
    "    print(\"Failed to extract EPUB text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5de0795f",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def process_search_result_documents(results: list[dict], max_documents: int = 3) -> dict[str, str]:\n",
    "    \"\"\"Process documents from search results and extract their text content.\n",
    "    \n",
    "    Args:\n",
    "        results: List of search results from search_pubs\n",
    "        max_documents: Maximum number of documents to process\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary mapping document URLs to their extracted text content\n",
    "    \"\"\"\n",
    "    processed_docs = {}\n",
    "    \n",
    "    for i, result in enumerate(results[:max_documents]):\n",
    "        doc_url = result['url']\n",
    "        file_format = result['file_format']\n",
    "        doc_number = result['document_number']\n",
    "        \n",
    "        print(f\"\\n--- Processing document {i+1}/{min(len(results), max_documents)}: {doc_number} ---\")\n",
    "        print(f\"Format: {file_format}\")\n",
    "        print(f\"URL: {doc_url}\")\n",
    "        \n",
    "        # Skip CAC-required documents\n",
    "        if \"Common Access Card (CAC)\" in result['description']:\n",
    "            print(\"Skipping CAC-required document\")\n",
    "            processed_docs[doc_url] = \"CAC_REQUIRED: This document requires Common Access Card (CAC) authentication\"\n",
    "            continue\n",
    "        \n",
    "        # Extract text content\n",
    "        text_content = await get_document_text_from_url(doc_url, file_format)\n",
    "        \n",
    "        if text_content:\n",
    "            processed_docs[doc_url] = text_content\n",
    "            print(f\"Successfully extracted {len(text_content)} characters\")\n",
    "        else:\n",
    "            processed_docs[doc_url] = \"EXTRACTION_FAILED: Could not extract text from this document\"\n",
    "            print(\"Failed to extract text\")\n",
    "    \n",
    "    return processed_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "069dcf6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Testing Comprehensive Document Processing ===\n",
      "\n",
      "--- Processing document 1/2: ARMY DIR 2018-18 ---\n",
      "Format: pdf\n",
      "URL: https://armypubs.army.mil/epubs/DR_pubs/DR_c/ARN13011-ARMY_DIR_2018-18-000-WEB-1.pdf\n",
      "Skipping CAC-required document\n",
      "\n",
      "--- Processing document 2/2: PPM CIO-024 ---\n",
      "Format: pdf\n",
      "URL: https://armypubs.army.mil/epubs/DR_pubs/DR_a/ARN41285-PPM_CIO-024-000-WEB-1.pdf\n",
      "(get_document_text_from_url) Downloading document: https://armypubs.army.mil/epubs/DR_pubs/DR_a/ARN41285-PPM_CIO-024-000-WEB-1.pdf\n",
      "(get_document_text_from_url) Content-Type: application/pdf\n",
      "(extract_document_text) Detected format: pdf\n",
      "(extract_pdf_text) Successfully extracted text from 8 pages\n",
      "Successfully extracted 13578 characters\n",
      "\n",
      "=== Processing Summary ===\n",
      "\n",
      "URL: https://armypubs.army.mil/epubs/DR_pubs/DR_c/ARN13011-ARMY_DIR_2018-18-000-WEB-1.pdf\n",
      "Status: CAC Required\n",
      "\n",
      "URL: https://armypubs.army.mil/epubs/DR_pubs/DR_a/ARN41285-PPM_CIO-024-000-WEB-1.pdf\n",
      "Status: Success - 13578 characters extracted\n",
      "Preview: DEPARTMENT OF THE ARMY  CHIEF INFORMATION OFFICER  107 ARMY PENTAGON  WASHINGTON DC  20310-0107  SAIS-ADS (25-1rrrr)   ADS-GOV-AI-024  MEMORANDUM FOR SEE DISTRIBUTION  SUBJECT: Chief Information Offic...\n"
     ]
    }
   ],
   "source": [
    "# Test comprehensive document processing with our AI search results\n",
    "print(\"=== Testing Comprehensive Document Processing ===\")\n",
    "if 'results' in locals():\n",
    "    # Process a few documents from our search results\n",
    "    processed = await process_search_result_documents(results, max_documents=2)\n",
    "    \n",
    "    print(f\"\\n=== Processing Summary ===\")\n",
    "    for url, content in processed.items():\n",
    "        print(f\"\\nURL: {url}\")\n",
    "        if content.startswith(\"CAC_REQUIRED\"):\n",
    "            print(\"Status: CAC Required\")\n",
    "        elif content.startswith(\"EXTRACTION_FAILED\"):\n",
    "            print(\"Status: Extraction Failed\")\n",
    "        else:\n",
    "            print(f\"Status: Success - {len(content)} characters extracted\")\n",
    "            # Show a preview of the content\n",
    "            preview = content[:200].replace('\\n', ' ').strip()\n",
    "            print(f\"Preview: {preview}...\")\n",
    "else:\n",
    "    print(\"No search results available. Run the search first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812684b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
